{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmaCOo/Assignment-1.1-Data-Acquisition-with-APIs-and-Scraping/blob/main/Text_Mining_API_and_Scrape_EmmaOo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SGKhzdXqUkM"
      },
      "source": [
        "##**ADS 509_ Assignment 1.1: Data Acquisition with APIs and Scraping**\n",
        "\n",
        "###**Emma Oo**\n",
        "###09/08/2022\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owdXvbmpqEqK"
      },
      "source": [
        "Acquiring data is one of the fundamental steps in any analysis and proficiency at APIs and web scraping unlocks rich data sets for future analyses. In this assignment you will pull data from the Twitter API and scrape lyrics from AZlyrics.com.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "079WwzJ6qolU"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "1. Create a repository under your GitHub account from this template. Instructions can be found here. \n",
        "NOTE: Make your repository public or add your instructor’s GitHub account as a collaborator.\n",
        "\n",
        "2. Choose two musical artists for your project. Both should have at least 100,000 followers on Twitter and at least twenty songs at their artist page on AZLyrics. Those pages have forms like this.\n",
        "\n",
        "3. The “API and Scrape.ipynb” file within the repository holds your starting code for the assignment.\n",
        "\n",
        "   a.  In the first part of the assignment, you will pull data from Twitter. Following the provided code and the instructions in Chapter 2 of your textbook, **pull the descriptions of all followers** for each of your artists.\n",
        "\n",
        "   b.  The second part of the file holds starting code for the second half of the assignment. Follow the instructions in that notebook, **pulling and storing the lyrics for both artists.**\n",
        "\n",
        "   c. The final part of the notebook holds evaluation code for the first two parts. If you’ve completed the assignment correctly, you should be able to just run this section as is. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNVTVbruqAqy",
        "outputId": "5cee7bf0-13ec-40bd-a298-158099289100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/tweepy/tweepy.git\n",
            "  Cloning https://github.com/tweepy/tweepy.git to /tmp/pip-req-build-5hl9lt05\n",
            "  Running command git clone -q https://github.com/tweepy/tweepy.git /tmp/pip-req-build-5hl9lt05\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.1) (3.2.0)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.1) (2.28.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.1) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "#install tweepy higher than 4.0\n",
        "!pip install git+https://github.com/tweepy/tweepy.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSzuUkm7tyOa",
        "outputId": "2895c7ab-03f0-4dfc-88e5-ae098b247957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tweepy\n",
            "Version: 4.10.1\n",
            "Summary: Twitter library for Python\n",
            "Home-page: https://www.tweepy.org/\n",
            "Author: Joshua Roesslein\n",
            "Author-email: tweepy@googlegroups.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: requests, oauthlib, requests-oauthlib\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show tweepy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAIxeJqIufGl"
      },
      "source": [
        "##**Twitter API Pull**\n",
        "###**ARTISTS:  ECOSMITH & SHAYNEWARD**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-M6ikLZDuYKU"
      },
      "outputs": [],
      "source": [
        "# for the twitter section\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "import os\n",
        "import datetime\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "# for the lyrics scrape section\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R95xPPQf4VD5"
      },
      "source": [
        "###**Twitter Keys & Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uRU4sNNG3FRv"
      },
      "outputs": [],
      "source": [
        "api_key = 'api-key' \n",
        "api_secret = 'api_secret_key'\n",
        "bearer_token = 'bearer_token'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TzoYGIFB3vgt"
      },
      "outputs": [],
      "source": [
        "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Bq2gcGs64CEs"
      },
      "outputs": [],
      "source": [
        "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM7rf6tU4cl0"
      },
      "source": [
        "###**Testing API**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOv7bHBYMgIp"
      },
      "source": [
        "The Twitter APIs are quite rich. Let's play around with some of the features before we dive into this section of the assignment. For our testing, it's convenient to have a small data set to play with. We will seed the code with the handle of John Chandler, one of the instructors in this course. His handle is @37chandler. Feel free to use a different handle if you would like to look at someone else's data.\n",
        "\n",
        "We will write code to explore a few aspects of the API:\n",
        "\n",
        "Pull some of the followers @37chandler.\n",
        "\n",
        "Explore response data, which gives us information about Twitter users.\n",
        "\n",
        "Pull the last few tweets by @37chandler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DgQdDKKOMgva"
      },
      "outputs": [],
      "source": [
        "handle = \"37chandler\"\n",
        "user_obj = client.get_user(username=handle)\n",
        "\n",
        "followers = client.get_users_followers(\n",
        "    # Learn about user fields here: \n",
        "    # https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/user\n",
        "    user_obj.data.id, user_fields=[\"created_at\",\"description\",\"location\",\n",
        "                                   \"public_metrics\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9R0GfL7Vo0W"
      },
      "source": [
        "Now let's explore these a bit. We'll start by printing out names, locations, following count, and followers count for these users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CfVtUrODVpcl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f65a34-7c12-49c5-f467-c6e24fc9ffd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dave Renn lists 'None' as their location.\n",
            " Following: 42, Followers: 10.\n",
            "\n",
            "Lionel lists 'None' as their location.\n",
            " Following: 202, Followers: 204.\n",
            "\n",
            "Megan Randall lists 'None' as their location.\n",
            " Following: 141, Followers: 100.\n",
            "\n",
            "Jacob Salzman lists 'None' as their location.\n",
            " Following: 562, Followers: 134.\n",
            "\n",
            "twiter not fun lists 'None' as their location.\n",
            " Following: 220, Followers: 21.\n",
            "\n",
            "Hariettwilsonincarnate lists 'None' as their location.\n",
            " Following: 218, Followers: 60.\n",
            "\n",
            "Christian Tinsley lists 'None' as their location.\n",
            " Following: 2, Followers: 0.\n",
            "\n",
            "Steve lists 'I'm over here.' as their location.\n",
            " Following: 1589, Followers: 33.\n",
            "\n",
            "John O'Connor 🇺🇦 lists 'None' as their location.\n",
            " Following: 8, Followers: 1.\n",
            "\n",
            "CodeGrade lists 'Amsterdam' as their location.\n",
            " Following: 2820, Followers: 424.\n",
            "\n",
            "Cleverhood lists 'Providence, RI' as their location.\n",
            " Following: 2793, Followers: 3560.\n",
            "\n",
            "Regina 🚶‍♀️🚲🌳 lists 'Minneapolis' as their location.\n",
            " Following: 2796, Followers: 3321.\n",
            "\n",
            "Eric Hallstrom lists 'Missoula, MT' as their location.\n",
            " Following: 464, Followers: 305.\n",
            "\n",
            "Tyler 📊 🐕 🚲 lists 'Minneapolis, MN' as their location.\n",
            " Following: 528, Followers: 83.\n",
            "\n",
            "The Center for Community Ownership (CCO) lists 'None' as their location.\n",
            " Following: 53, Followers: 41.\n",
            "\n",
            "Deepak Chauhan lists 'None' as their location.\n",
            " Following: 452, Followers: 25.\n",
            "\n",
            "Patsy lists 'Seattle, WA' as their location.\n",
            " Following: 156, Followers: 15.\n",
            "\n",
            "andrew lists 'St Paul, MN' as their location.\n",
            " Following: 1415, Followers: 461.\n",
            "\n",
            "Ada Smith lists 'None' as their location.\n",
            " Following: 274, Followers: 198.\n",
            "\n",
            "Stacey Burns lists 'Minneapolis Witch District' as their location.\n",
            " Following: 4587, Followers: 10881.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_to_print = 20\n",
        "\n",
        "for idx, user in enumerate(followers.data) :\n",
        "    following_count = user.public_metrics['following_count']\n",
        "    followers_count = user.public_metrics['followers_count']\n",
        "    \n",
        "    print(f\"{user.name} lists '{user.location}' as their location.\")\n",
        "    print(f\" Following: {following_count}, Followers: {followers_count}.\")\n",
        "    print()\n",
        "    \n",
        "    if idx >= (num_to_print - 1) :\n",
        "        break\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq47V7NoVuav"
      },
      "source": [
        "Let's find the person who follows this handle who has the most followers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sYm5GmwtVsC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a290db2-a728-4664-9ce1-76958b96fa6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WedgeLIVE\n",
            "{'followers_count': 14170, 'following_count': 2222, 'tweet_count': 56102, 'listed_count': 218}\n"
          ]
        }
      ],
      "source": [
        "max_followers = 0\n",
        "\n",
        "for idx, user in enumerate(followers.data) :\n",
        "    followers_count = user.public_metrics['followers_count']\n",
        "    \n",
        "    if followers_count > max_followers :\n",
        "        max_followers = followers_count\n",
        "        max_follower_user = user\n",
        "\n",
        "        \n",
        "print(max_follower_user)\n",
        "print(max_follower_user.public_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZNUp9GsVwzt"
      },
      "source": [
        "Let's pull some more user fields and take a look at them. The fields can be specified in the user_fields argument.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "103VAPgaV0bm"
      },
      "outputs": [],
      "source": [
        "response = client.get_user(id=user_obj.data.id,\n",
        "                          user_fields=[\"created_at\",\"description\",\"location\",\n",
        "                                       \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
        "                                       \"verified\",\"public_metrics\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "371qtgmAV2ow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b501e49c-c423-4d19-b929-c6cfd378c78a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for description we have He/Him. Data scientist, urban cyclist, educator, erstwhile frisbee player. \n",
            "\n",
            "¯\\_(ツ)_/¯\n",
            "for id we have 33029025\n",
            "for verified we have False\n",
            "for location we have MN\n",
            "for profile_image_url we have https://pbs.twimg.com/profile_images/2680483898/b30ae76f909352dbae5e371fb1c27454_normal.png\n",
            "for name we have John Chandler\n",
            "for public_metrics we have {'followers_count': 193, 'following_count': 589, 'tweet_count': 996, 'listed_count': 3}\n",
            "for created_at we have 2009-04-18 22:08:22+00:00\n",
            "for username we have 37chandler\n"
          ]
        }
      ],
      "source": [
        "for field, value in response.data.items() :\n",
        "    print(f\"for {field} we have {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpvvh0lnV6Uk"
      },
      "source": [
        "Now a few questions for you about the user object.\n",
        "\n",
        "Q: How many fields are being returned in the response object?\n",
        "\n",
        "A: 9\n",
        "\n",
        "\n",
        "Q: Are any of the fields within the user object non-scalar? (I.e., more complicated than a simple data type like integer, float, string, boolean, etc.)\n",
        "\n",
        "A: Public metrics have more than one data type.\n",
        "\n",
        "\n",
        "Q: How many friends, followers, and tweets does this user have?\n",
        "\n",
        "A: 589 friends, 193 followers, and 994 tweet counts. \n",
        "\n",
        "Although you won't need it for this assignment, individual tweets can be a rich source of text-based data. To illustrate the concepts, let's look at the last few tweets for this user. You are encouraged to explore the fields that are available about Tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LHS8A_CMWAiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281ff9e2-df6f-4551-80ea-1fd9ea3bbeac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1569155273742327811\n",
            "As a Minneapolis person, I knew we had Toronto beat, but I didn't realize Portland had us beat: https://t.co/xrx5mOFcWK.\n",
            "\n",
            "But @nytimes, c'mon! https://t.co/M9mBWhdgsj\n",
            "\n",
            "1568982292923826176\n",
            "RT @wonderofscience: Amazing lenticular cloud over Mount Fuji\n",
            "\n",
            "Credit: Iurie Belegurschi\n",
            "https://t.co/0mUxl28H9U\n",
            "\n",
            "1568242374085869570\n",
            "RT @depthsofwiki: lots of memes about speedy wikipedia editors — quick thread about what went down on wikipedia in the minutes after her de…\n",
            "\n",
            "1568074978754703361\n",
            "@DrLaurenWilson @leighradwood @MaritsaGeorgiou @Walgreens I could not possibly agree more with this sentiment. Compared to almost any other primary care I've received, they are great.\n",
            "\n",
            "1567530169686196224\n",
            "@DrLaurenWilson @MaritsaGeorgiou @Walgreens For those who have access to Curry Health Center on campus, you can get a bivalent booster in 15 minutes from their delightful staff.\n",
            "\n",
            "1567511181526708224\n",
            "RT @shes_the_maNN1: I can’t describe how ancient this makes me feel. https://t.co/a1IvELjOFY\n",
            "\n",
            "1567510612665864193\n",
            "RT @AngryBlackLady: this is hilarious\n",
            "\n",
            "1566031636457725953\n",
            "RT @MarkJacob16: With all the arguments over whether MAGA Republicans are fascists, I reread William Shirer’s “The Rise and Fall of the Thi…\n",
            "\n",
            "1563737816219000832\n",
            "RT @wonderofscience: The Milky Way galaxy and a phenomenon known as \"airglow\" seen from the International Space Station. https://t.co/bOLt8…\n",
            "\n",
            "1563243769209954305\n",
            "Such an awesome idea from @EmilyMandel to write some bonus content for “Sea of Tranquility” editions sold at independent book stores. Fantastic book, too. 👏👏👏 https://t.co/EMGpCEsKTn\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = client.get_users_tweets(user_obj.data.id)\n",
        "\n",
        "# By default, only the ID and text fields of each Tweet will be returned\n",
        "for idx, tweet in enumerate(response.data) :\n",
        "    print(tweet.id)\n",
        "    print(tweet.text)\n",
        "    print()\n",
        "    \n",
        "    if idx > 10 :\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AX-Iwld6w4G"
      },
      "source": [
        "###**Pulling Follower Information**\n",
        "\n",
        "\n",
        "In this next section of the assignment, we will pull information about the followers of your two artists. We've seen above how to pull a set of followers using **client.get_users_followers**. This function has a parameter, max_results, that we can use to change the number of followers that we pull. Unfortunately, we can only pull 1000 followers at a time, which means we will need to handle the pagination of our results.\n",
        "\n",
        "The return object has the .data field, where the results will be found. It also has .meta, which we use to select the next \"page\" in the results using the next_token result. I will illustrate the ideas using our user from above.\n",
        "\n",
        "###Rate Limiting\n",
        "Twitter limits the rates at which we can pull data, as detailed in this guide. We can make 15 user requests per 15 minutes, meaning that we can pull  users per hour. I illustrate the handling of rate limiting below, though whether or not you hit that part of the code depends on your value of handle.\n",
        "\n",
        "In the below example, I'll pull all the followers, 25 at a time. (We're using 25 to illustrate the idea; when you do this set the value to 1000.)\n",
        "\n",
        "TODO: Get artist counts and estimate total time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSJrBwh0aSh2"
      },
      "source": [
        "Now let's take a look at your artists and see how long it is going to take to pull all their followers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "UOgUONLpaRki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace49851-07ac-4667-8c38-40b77d2d4906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It would take 2.88 hours to pull all 172555 followers for echosmith. \n",
            "It would take 5.82 hours to pull all 349148 followers for shayneTward. \n"
          ]
        }
      ],
      "source": [
        "artists = dict()\n",
        "\n",
        "for handle in ['echosmith','shayneTward'] : \n",
        "    user_obj = client.get_user(username=handle,user_fields=[\"public_metrics\"])\n",
        "    artists[handle] = (user_obj.data.id, \n",
        "                       handle,\n",
        "                       user_obj.data.public_metrics['followers_count'])\n",
        "    \n",
        "\n",
        "for artist, data in artists.items() : \n",
        "    print(f\"It would take {data[2]/(1000*15*4):.2f} hours to pull all {data[2]} followers for {artist}. \")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkwBK7q3aXtl"
      },
      "source": [
        "Depending on what you see in the display above, you may want to limit how many followers you pull. It'd be great to get at least 200,000 per artist.\n",
        "\n",
        "As we pull data for each artist we will write their data to a folder called \"twitter\", so we will make that folder if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a5wtgBJ_aZxR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Make the \"twitter\" folder here. If you'd like to practice your programming, add functionality \n",
        "# that checks to see if the folder exists. If it does, then \"unlink\" it. Then create a new one.\n",
        "\n",
        "if os.path.isdir(\"twitter\"):\n",
        "    shutil.rmtree(\"twitter/\")\n",
        "os.mkdir(\"twitter\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHM2spypQer-"
      },
      "source": [
        "In this following cells, build on the above code to pull some of the followers and their data for your two artists. As you pull the data, write the follower ids to a file called [artist name]_followers.txt in the \"twitter\" folder. For instance, for Cher I would create a file named cher_followers.txt. As you pull the data, also store it in an object like a list or a data frame.\n",
        "\n",
        "Extract and store the following fields:\n",
        "\n",
        "screen_name\n",
        "\n",
        "name\n",
        "\n",
        "id\n",
        "\n",
        "location\n",
        "\n",
        "followers_count\n",
        "\n",
        "friends_count\n",
        "\n",
        "description\n",
        "\n",
        "Store the fields with one user per row in a tab-delimited text file with the name [artist name]_follower_data.txt. For instance, for Cher I would create a file named cher_follower_data.txt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Scraping for Artist_1: Ecosmith**"
      ],
      "metadata": {
        "id": "6m2ZY-aUjxvZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "L01NJW-sdfQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82db95c-fdfe-4fa0-a347-f0bc21024db8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'result_count': 1000, 'next_token': 'BLE5JOD3C0D1GZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'FM62C5UMDG11GZZZ', 'previous_token': 'LSAUANN3JVIUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'BR2CUQTBJJNHEZZZ', 'previous_token': '2R24A7OVIJUUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '40PAGGOQ0V7HEZZZ', 'previous_token': 'C8SL8R0OFS8EGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '92JO19E0DMQ1EZZZ', 'previous_token': '1P0JFT7JVCOEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'IGB44E0DPA1HEZZZ', 'previous_token': 'O2H05GFLK55UGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NVA79CKKJPD1EZZZ', 'previous_token': '7NJ53A3C75UEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'C5G6Q8S87KOHEZZZ', 'previous_token': 'VT3P7MSND6IUGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'HAVMA4MSM0F1EZZZ', 'previous_token': 'NQDJKG4IOB7EGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'PTKN0NASFO3HEZZZ', 'previous_token': '527GBHIKA7GUGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'FD44T60C87KHCZZZ', 'previous_token': 'K9IR9R7EGNSEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'RLBILQ1DLB9HCZZZ', 'previous_token': 'RP1PRSISOKBEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'M9BIB5H61URHCZZZ', 'previous_token': '8QFM8I7TAKMEIZZZ'}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 888 seconds.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'result_count': 1000, 'next_token': 'GI0L95PVAUEHCZZZ', 'previous_token': 'JHM3JUFT0H4UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '8KU4LHV0HI31CZZZ', 'previous_token': 'P8RLCHISM1HEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'P1SID1547TP1CZZZ', 'previous_token': '2OVB4LUHFHSUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'JJBUHF760DGHCZZZ', 'previous_token': 'TQ3CUASUOU6UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'FCDOOGP991CHCZZZ', 'previous_token': '2TOF2RSJVQFEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'O8VIQ8QL2D71CZZZ', 'previous_token': 'REDQVAO3N2JEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'AUD13J8PT50HCZZZ', 'previous_token': 'L9GT1GCUU2OUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'MEM76GGUOCTHCZZZ', 'previous_token': 'D9O74QP72UVEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'ICM8BSRK5ORHCZZZ', 'previous_token': 'IL82VQ4T7R2EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'SK2S96BP80QHCZZZ', 'previous_token': 'MNU3PRTMQ74EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'TVA2OI3RN8Q1CZZZ', 'previous_token': 'M13SPPCMNV5EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'HM93FI0RHKQ1CZZZ', 'previous_token': 'KR1MK64F8N5UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'C8UHNC96DGQ1CZZZ', 'previous_token': 'OGVTKHF8EB5UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'KTQ68ESGCOQ1CZZZ', 'previous_token': 'NFLKIJMQIF5UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '35S0ACRL7OQ1CZZZ', 'previous_token': '4K6O3TBGJ75UIZZZ'}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 888 seconds.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'result_count': 1000, 'next_token': 'R1EUDPNNVCPHCZZZ', 'previous_token': 'EJQL44M0O75UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'UBQS6V6MP8PHCZZZ', 'previous_token': 'V9VBJBG90J6EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CH5QL8GTOKPHCZZZ', 'previous_token': 'KB4FU7PF6N6EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'H8TGDJVFO8PHCZZZ', 'previous_token': '5AV6C8737B6EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'GEABEBCBMOOHCZZZ', 'previous_token': 'QQRQGFOG7N6EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6OPJ1JBSHKLHCZZZ', 'previous_token': 'VPHPVS3L977EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'PMTN6D177GJ1CZZZ', 'previous_token': '010JUFH7EJAEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6M34OO808CH1CZZZ', 'previous_token': 'NVRT1JVROFCUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'I4M8F43IIKEHCZZZ', 'previous_token': '0BBR3VCLNNEUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '91HL5K8LBGC1CZZZ', 'previous_token': 'OOOI389SDFHEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '3EPS02P18S91CZZZ', 'previous_token': 'MPH1EHGJKNJUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'M9NJUHOEJS6HCZZZ', 'previous_token': '3HQB087LN7MUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'UV83D13IJ45HCZZZ', 'previous_token': 'DO42TJBAC7PEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'O8G5LN22PS41CZZZ', 'previous_token': 'C7B798KJCRQEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '1HR0HOMGJ42HCZZZ', 'previous_token': 'G0DVATGR67RUIZZZ'}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 889 seconds.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'result_count': 1000, 'next_token': '00GVHAS0HC11CZZZ', 'previous_token': 'QNUG5OUQCRTEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'LCKUJ9FCMBTHAZZZ', 'previous_token': '14JPOQENEJUUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'P34229QMUJQ1AZZZ', 'previous_token': 'JE77PUH89K2EKZZZ'}\n",
            "{'result_count': 999, 'next_token': 'U4SJ7QKSRRNHAZZZ', 'previous_token': '3135B38J1G5UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5QQNFVDDVJLHAZZZ', 'previous_token': '432SN468488EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'JD5L0SQB6NL1AZZZ', 'previous_token': 'BA69N5TB0CAEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '92PN14NPMVJ1AZZZ', 'previous_token': 'EDHKD632PCAUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'MFHA9AICCRH1AZZZ', 'previous_token': '3KR116T09GCUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5A3IL32L7BEHAZZZ', 'previous_token': 'JO9NN4N8J8EUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'TLA7238SR7BHAZZZ', 'previous_token': '1791JSDUP4HEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'TK42BDN9PV91AZZZ', 'previous_token': 'T9MTFQ644SKEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CT28K9EABR71AZZZ', 'previous_token': 'GE434EK560MUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'RFMJUR904N61AZZZ', 'previous_token': 'G36IF8HRK4OUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'N2OU81MNFJ51AZZZ', 'previous_token': 'USAQJJE2RCPUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '33SLCOIGLN41AZZZ', 'previous_token': '7EHVR41CGCQUKZZZ'}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 889 seconds.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'result_count': 1000, 'next_token': '4UMSI4VB671HAZZZ', 'previous_token': 'N2CNQ5SSACRUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'I16AQ6O72IVHAZZZ', 'previous_token': 'FU7NOOBEQCUEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '0NDJQTFIM6THAZZZ', 'previous_token': '806V2NGETH0EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'GOROAGJOPASHAZZZ', 'previous_token': 'TANPNGI69P2EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'TV7BL9S9EIS1AZZZ', 'previous_token': '3NE2HUO36P3EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'M8AHGH4N86R1AZZZ', 'previous_token': 'F8IG80C1HD3UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '7GS55QQ1JMQ1AZZZ', 'previous_token': 'CMJIN27RNT4UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '0QJTTK8P9IPHAZZZ', 'previous_token': '7OBHEGN5C95UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'BEQLRHGM9UNHAZZZ', 'previous_token': 'G8UD81VNMD6EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'QTC8UH4IBQMHAZZZ', 'previous_token': 'IDM7V4OPM98EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '8CQB138N8AM1AZZZ', 'previous_token': '76A1A3MTK99EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'OUBETAQMK2L1AZZZ', 'previous_token': 'DGJ6HGG9NP9UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'B8SVCJQ3JIKHAZZZ', 'previous_token': 'OR0GP0TABTAUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '44Q8EKUUKAK1AZZZ', 'previous_token': 'MPAAI4UCCDBEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'MTJK4ATC4IJHAZZZ', 'previous_token': 'BE7PE2SEBLBUKZZZ'}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 889 seconds.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'result_count': 1000, 'next_token': 'HJCIL88LV6GHAZZZ', 'previous_token': 'AE0J3VJ5RDCEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NS7K6VQUFUEHAZZZ', 'previous_token': 'D3BKSAJU0TFEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'F175P79MSED1AZZZ', 'previous_token': '89PDPGEFG1HEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'VENURR1JSQC1AZZZ', 'previous_token': 'ROR06OUC3LIUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '7DO5TQ9M8MBHAZZZ', 'previous_token': 'A2OB1O2T39JUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'JPTJF5TOUUAHAZZZ', 'previous_token': 'U9KKTFFGN9KEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5AML1FQBCEA1AZZZ', 'previous_token': '91TEHTJH11LEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'DL8BUPMULU91AZZZ', 'previous_token': '02V20RUTJHLUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'HSE4VQ913U81AZZZ', 'previous_token': 'O3SJOPMIA1MUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'HECQUNPSJU61AZZZ', 'previous_token': '69S6NCJQS5NUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9VHRVA78FU41AZZZ', 'previous_token': 'J70ET2MTC1PUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '1JR4J85OTM21AZZZ', 'previous_token': 'OHCOA72TG5RUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'RJN3OQTNTQ0HAZZZ', 'previous_token': 'DH4G013A29TUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '0HGSNV975A01AZZZ', 'previous_token': '4IK6DMTU25VEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'I77VKVVSIHV1AZZZ', 'previous_token': 'SJAK3LNCQLVUKZZZ'}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 888 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '737JJRCKHTUHAZZZ', 'previous_token': 'UL55RR8ADE0UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'FS3PUFQ8A5U1AZZZ', 'previous_token': 'P17NH4ROE21EKZZZ'}\n",
            "{'result_count': 999, 'next_token': 'B6FQGKAQT9T1AZZZ', 'previous_token': '4M69M6V2LQ1UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '02P51KA7H1SHAZZZ', 'previous_token': '74QKJV052U2UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '8O58I3URC5S1AZZZ', 'previous_token': '7NSMV0RBF23EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'GEB597K65TRHAZZZ', 'previous_token': 'I2SUKB9AJQ3UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '4MBTVGURK1QHAZZZ', 'previous_token': 'JE1R8K3QQ24EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'SIVHHL7FPHPHAZZZ', 'previous_token': 'U1BG7PKBC65EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'LRVORESVVTOHAZZZ', 'previous_token': '4FBK7NIH6E6EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'I18RTD9PHTO1AZZZ', 'previous_token': '74MKMR3L027EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'C2EQS60I19NHAZZZ', 'previous_token': 'R4R1IMH7E67UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'MHKMV70JRHM1AZZZ', 'previous_token': 'NC8CR7Q0UQ8EKZZZ'}\n",
            "1:30:16.268711\n"
          ]
        }
      ],
      "source": [
        "# Modify the below code stub to pull the follower IDs and write them to a file. \n",
        "\n",
        "handles = ['ecosmith']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit=100):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    df = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    df2 = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    df.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    df2.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Scraping for Artist_2: ShayneWard**"
      ],
      "metadata": {
        "id": "aPTLP5Ovk-lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the below code stub to pull the follower IDs and write them to a file. \n",
        "\n",
        "handles = ['shayneTward']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit=100):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count', 'description']\n",
        "    columns_3 = ['ID']\n",
        "    df3 = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    df4 = pd.DataFrame(followers_data[handle], columns = columns_3)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    df3.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    df4.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No-_zMuZSi2s",
        "outputId": "ab3fc17d-5180-4ea0-d4d8-89beedd7d4a8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'RLHD2NQ8CCD1GZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'SRGHEO3VE411GZZZ', 'previous_token': 'KEHQC7ISJVIUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'ET18NQ0QKFNHEZZZ', 'previous_token': 'GDPTJQ19IFUUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '3G13DRU50V7HEZZZ', 'previous_token': 'K8TJ152KCC8EGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'GJCDQGO4JQQ1EZZZ', 'previous_token': 'S36LFF75V0OEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'C21H51UDTI1HEZZZ', 'previous_token': 'N1C7UMHVI95UGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'PO6Q1M4LJPD1EZZZ', 'previous_token': 'DJKRRHVI6LUEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '7CVHRS927OOHEZZZ', 'previous_token': '84LOMJBBC6IUGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '0F0VKD4EMGF1EZZZ', 'previous_token': 'JUFP5N3NOB7EGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5RNTVHH0H03HEZZZ', 'previous_token': 'EP09LR939VGUGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5A65J68T9NKHCZZZ', 'previous_token': '66B8V8L3G7SEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '2COE6IQ8LN9HCZZZ', 'previous_token': 'GMRR2PVJNOBEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'OCATU8PV26RHCZZZ', 'previous_token': '4EKDA5UIAKMEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'PSAD76BIBEEHCZZZ', 'previous_token': '9QKDKQEPU14EIZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 889 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'P3QEPCE5HM31CZZZ', 'previous_token': 'FHVAMQ60L1HEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'E9IJOG9S8LP1CZZZ', 'previous_token': 'NF1RAE0VEDSUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'M9EKKLEG0LGHCZZZ', 'previous_token': '723DIUQRO26UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '782P042S91CHCZZZ', 'previous_token': 'CGK1EGOPVIFEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9NUMMC442H71CZZZ', 'previous_token': 'GNI77F6MMUJEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'I6BOF7HAT50HCZZZ', 'previous_token': '7R0D5N5ATIOUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'VLAUC7NFOGTHCZZZ', 'previous_token': 'L5IUSCN62QVEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5AC6LF6E5SRHCZZZ', 'previous_token': 'VE13D4UG7F2EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '3JQNJLTD84QHCZZZ', 'previous_token': 'JCTKDBJMQ34EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'T7117PKUN8Q1CZZZ', 'previous_token': 'IMADCU2PNV5EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '1Q8S9SHDHKQ1CZZZ', 'previous_token': 'KQFP173N8N5UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CRITSC19DGQ1CZZZ', 'previous_token': 'J4S5VOMVEB5UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'GUO1R24JCOQ1CZZZ', 'previous_token': 'O1HGEK6OIF5UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '8LEC3ISS7OQ1CZZZ', 'previous_token': '6LSFCARFJ75UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9I45U88FVGPHCZZZ', 'previous_token': 'KDF9MP40O75UIZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 889 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'EQNCFPUSP8PHCZZZ', 'previous_token': 'N6RDSAG30J6EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'FNPMP98UOKPHCZZZ', 'previous_token': 'NOPCC9P76N6EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '3RQ1KI7GO8PHCZZZ', 'previous_token': 'UE82IU717B6EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'HUSBD7ENMOOHCZZZ', 'previous_token': 'BEL8O0OG7N6EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NKPVAQQAI0LHCZZZ', 'previous_token': '7LAJCQA6977EIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'PP7T5BEU7GJ1CZZZ', 'previous_token': '9RHRP0QMEBAEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'K1NEQTV58GH1CZZZ', 'previous_token': 'IUNPMMS1OFCUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'OHEF5DQTIOEHCZZZ', 'previous_token': 'R05HCS9LNFEUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'ASN8NIKDBGC1CZZZ', 'previous_token': 'NF8IMBM1D7HEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'BOASP0J18S91CZZZ', 'previous_token': 'V9J2TV4KKFJUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '1EL3EA24K86HCZZZ', 'previous_token': '2BQH0TEUN3MUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6CHC9LCPJG5HCZZZ', 'previous_token': '6ODA6G5CC3PEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'VB8VQJ7CQC41CZZZ', 'previous_token': 'JAH4HC6OCNQEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'TQHM2OJPJ82HCZZZ', 'previous_token': 'EJUM0OBG5RRUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'QFOPL78SHK11CZZZ', 'previous_token': 'TGQ8EIVBCNTEIZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 890 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'ET7UVL5PMNTHAZZZ', 'previous_token': 'IB641537EFUUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'EQGS6EKRURQ1AZZZ', 'previous_token': 'S9HFLDP59G2EKZZZ'}\n",
            "{'result_count': 999, 'next_token': '2VCJAKUGRRNHAZZZ', 'previous_token': 'DEPODH121C5UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'HQMESPF2VJLHAZZZ', 'previous_token': 'B5I9KKHP448EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'OKRAR89I6RL1AZZZ', 'previous_token': 'SHM4D19U0CAEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9UUP96E3NFJ1AZZZ', 'previous_token': '9SBRQMQRP8AUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CN2EQEFPE7H1AZZZ', 'previous_token': 'OJ4QP7M88OCUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NNEORLV3AREHAZZZ', 'previous_token': '6C3FKSLAHOEUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'IDJPKVSJRFBHAZZZ', 'previous_token': '2SCUMUJPM4HEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6KGRPIH5Q791AZZZ', 'previous_token': 'U2BO5QIB4KKEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CN169PMPBR71AZZZ', 'previous_token': 'LQSNQP9U5SMUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '19N5687D4N61AZZZ', 'previous_token': 'MCU7TL9KK4OUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'DRIU7KIUFN51AZZZ', 'previous_token': 'QEV1SVLNR8PUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'S65E4K17MV41AZZZ', 'previous_token': 'OV7Q06N3G8QUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '7MUF1E4J6B1HAZZZ', 'previous_token': '8GMKCSHD9GRUKZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 889 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'RN9P302E2MVHAZZZ', 'previous_token': 'SHN07IMQPKUEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'A7I1FU6SMATHAZZZ', 'previous_token': 'OB6V310CTD0EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '956T9QS1PESHAZZZ', 'previous_token': 'I4E0II8A9P2EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'TG6AHT4HEIS1AZZZ', 'previous_token': '7R7LVPTH6H3EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'JLJGQJFJ86R1AZZZ', 'previous_token': 'STCLV63HHD3UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'UCQKK7FLJMQ1AZZZ', 'previous_token': '9RLEFER8NP4UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'VNAGU2HH9IPHAZZZ', 'previous_token': 'OJ3QQ55UC95UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '53AU73SR9UNHAZZZ', 'previous_token': 'V9C22BN6MD6EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CCMP9KCUBUMHAZZZ', 'previous_token': 'KL5A4EF9M18EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'MSIR6GKS8AM1AZZZ', 'previous_token': '56JN1ERDK59EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'QMOS77JMK2L1AZZZ', 'previous_token': 'NN5KUSN8NL9UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'AHQQ252NJIKHAZZZ', 'previous_token': '75KH2L59BTAUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'AD0KQ3VRKAK1AZZZ', 'previous_token': 'KR30JC5SCDBEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'L267FRLT4MJHAZZZ', 'previous_token': 'RV5NHB11BLBUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '128SPMH8V6GHAZZZ', 'previous_token': '96CBRL2JRDCEKZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 890 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '67CTK8JMFUEHAZZZ', 'previous_token': 'EGJDANNA0PFEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'D00JPS2ASED1AZZZ', 'previous_token': '87OBP051G1HEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '8TB8UDTDSQC1AZZZ', 'previous_token': 'H2OQ6OM93HIUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'K0LIDERR8MBHAZZZ', 'previous_token': '0L8144UC35JUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CCQUN20RV2AHAZZZ', 'previous_token': 'OM7Q25M9N9KEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'Q8CSADKVCEA1AZZZ', 'previous_token': 'CA2CGQ2711LEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '1K1F0RCUM691AZZZ', 'previous_token': 'QP9AUG5KJHLUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'F94B7E9R3U81AZZZ', 'previous_token': 'IENK1691A1MUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'VBLSD043JU61AZZZ', 'previous_token': 'E7HR05MUS1NUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'ASL7DF9TG641AZZZ', 'previous_token': 'ELJ51863C1PUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'I9GCJ6F2TM21AZZZ', 'previous_token': 'M4E40LONG1RUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'MO9PCESVTU0HAZZZ', 'previous_token': 'UG4RCNQ729TUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NEKQQLR25A01AZZZ', 'previous_token': '4G8S752825VEKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'QOH15668ILV1AZZZ', 'previous_token': 'VIF380MOQLVUKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'FSQOG3KMHTUHAZZZ', 'previous_token': 'DSO0B003DE0UKZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 889 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '37E6T2R2A5U1AZZZ', 'previous_token': 'P0OCC4JBE21EKZZZ'}\n",
            "{'result_count': 999, 'next_token': 'GGVDLORMT9T1AZZZ', 'previous_token': 'G7S61G5NLQ1UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'SBMM57BLH5SHAZZZ', 'previous_token': 'KTG5FBL52M2UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'E9KI0AV3C5S1AZZZ', 'previous_token': '017AUBLOEU3EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'MB68M2LF5TRHAZZZ', 'previous_token': 'NBQNDS14JQ3UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'Q5KB0FIVKHQHAZZZ', 'previous_token': 'FLKQMOBPQ24EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CPD6IPNRPHPHAZZZ', 'previous_token': 'RDK20F14BU5EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5BLQPF6AVTOHAZZZ', 'previous_token': '3H0EEAOG6E6EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '57BUR635HTO1AZZZ', 'previous_token': 'A8074H30027EKZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'SQ0P3V2Q19NHAZZZ', 'previous_token': 'E2N42IM6E27UKZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9AKS6J1QRHM1AZZZ', 'previous_token': 'K1H53PVDUM8EKZZZ'}\n",
            "1:30:16.064034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "tVkbFdTAWgAI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6769d5d4-448d-4b5d-cec6-ddf80452800f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Home by Warsan Shire no one leaves home unless home is the mouth of a shark. you only run for the border when you see the whole city running as well.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "tricky_description = \"\"\"\n",
        "    Home by Warsan Shire\n",
        "    \n",
        "    no one leaves home unless\n",
        "    home is the mouth of a shark.\n",
        "    you only run for the border\n",
        "    when you see the whole city\n",
        "    running as well.\n",
        "\n",
        "\"\"\"\n",
        "# This won't work in a tab-delimited text file.\n",
        "\n",
        "clean_description = re.sub(r\"\\s+\",\" \",tricky_description).strip()\n",
        "clean_description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQiJrv8LYigV"
      },
      "source": [
        "##**Lyrics Scraping**\n",
        "\n",
        "This section asks you to pull data from the Twitter API and scrape www.AZLyrics.com. In the notebooks where you do that work you are asked to store the data in specific ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "tHmHUDIsWiJj"
      },
      "outputs": [],
      "source": [
        "artists = {'EcoSmith':\"https://www.azlyrics.com/e/echosmith.html\",\n",
        "    'ShayneWard':\"https://www.azlyrics.com/s/shayneward.html\"} \n",
        "# we'll use this dictionary to hold both the artist name and the link on AZlyrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSMADexjY8ry"
      },
      "source": [
        "###**A Note on Rate Limiting**\n",
        "\n",
        "The lyrics site, www.azlyrics.com, does not have an explicit maximum on number of requests in any one time, but in our testing it appears that too many requests in too short a time will cause the site to stop returning lyrics pages. (Entertainingly, the page that gets returned seems to only have the song title to a Tom Jones song.)\n",
        "\n",
        "Whenever you call requests.get to retrieve a page, put a time.sleep(5 + 10*random.random()) on the next line. This will help you not to get blocked. If you do get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3pB8UCqqD-w"
      },
      "source": [
        "###**Part 1: Finding Links to Songs Lyrics**\n",
        "\n",
        "That general artist page has a list of all songs for that artist with links to the individual song pages.\n",
        "\n",
        "Q: Take a look at the robots.txt page on www.azlyrics.com. (You can read more about these pages here.) Is the scraping we are about to do allowed or disallowed by this page? How do you know?\n",
        "\n",
        "**A:  Scraping is allowed with Robots.txt.  But anything that include the /lyricsdb/ and the/song/paths are disallowed.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KqjvmvU993uc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Let's set up a dictionary of lists to hold our links\n",
        "lyrics_pages = defaultdict(list)\n",
        "\n",
        "for artist, artist_page in artists.items() :\n",
        "    # request the page and sleep\n",
        "    r = requests.get(artist_page)\n",
        "    time.sleep(5 + 10*random.random())\n",
        "\n",
        "    # now extract the links to lyrics pages from this page\n",
        "    soup = BeautifulSoup(r.content)\n",
        "    links = soup.select('a[href^=\"/lyrics/\"]')\n",
        "    # store the links `lyrics_pages` where the key is the artist and the\n",
        "    # value is a list of links. \n",
        "    lyrics_pages[artist] = [\"https://www.azlyrics.com\" + i.get(\"href\") for i in links]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB_Tnir--J1t"
      },
      "source": [
        "Let's make sure we have enough lyrics pages to scrape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "lMvJeW-8-MAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e401d81-26e5-4a86-fa80-42c5ccf23438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For EcoSmith we have 42.\n",
            "The full pull will take for this artist will take 0.12 hours.\n",
            "For ShayneWard we have 59.\n",
            "The full pull will take for this artist will take 0.16 hours.\n"
          ]
        }
      ],
      "source": [
        "# Let's see how long it's going to take to pull these lyrics \n",
        "# if we're waiting `5 + 10*random.random()` seconds \n",
        "for artist, links in lyrics_pages.items() : \n",
        "    print(f\"For {artist} we have {len(links)}.\")\n",
        "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_a4Nn1d0z4r"
      },
      "source": [
        "###**Part 2: Pulling Lyrics**\n",
        "\n",
        "Now that we have the links to our lyrics pages, let's go scrape them! Here are the steps for this part.\n",
        "\n",
        "Create an empty folder in our repo called \"lyrics\".\n",
        "\n",
        "Iterate over the artists in lyrics_pages.\n",
        "\n",
        "Create a subfolder in lyrics with the artist's name. For instance, if the artist was Cher you'd have lyrics/cher/ in your repo.\n",
        "\n",
        "Iterate over the pages.\n",
        "\n",
        "Request the page and extract the lyrics from the returned HTML file using BeautifulSoup.\n",
        "\n",
        "Use the function below, generate_filename_from_url, to create a filename based on the lyrics page, then write the lyrics to a text file with that name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "zswqIiay0-D9"
      },
      "outputs": [],
      "source": [
        "def generate_filename_from_link(link) :\n",
        "    \n",
        "    if not link :\n",
        "        return None\n",
        "    \n",
        "    # drop the http or https and the html\n",
        "    name = link.replace(\"https\",\"\").replace(\"http\",\"\")\n",
        "    name = link.replace(\".html\",\"\")\n",
        "\n",
        "    name = name.replace(\"/lyrics/\",\"\")\n",
        "    \n",
        "    # Replace useless chareacters with UNDERSCORE\n",
        "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
        "    \n",
        "    # tack on .txt\n",
        "    name = name + \".txt\"\n",
        "    \n",
        "    return(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "erStwYgl1CkK"
      },
      "outputs": [],
      "source": [
        "# Make the lyrics folder here. If you'd like to practice your programming, add functionality \n",
        "# that checks to see if the folder exists. If it does, then use shutil.rmtree to remove it and create a new one.\n",
        "\n",
        "if os.path.isdir(\"lyrics\") : \n",
        "    shutil.rmtree(\"lyrics/\")\n",
        "\n",
        "os.mkdir(\"lyrics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3hda1p4a1jZG"
      },
      "outputs": [],
      "source": [
        "url_stub = \"https://www.azlyrics.com\" \n",
        "start = time.time()\n",
        "\n",
        "total_pages = 0 \n",
        "\n",
        "for artist in lyrics_pages :\n",
        "    # Use this space to carry out the following steps: \n",
        "    \n",
        "    # 1. Build a subfolder for the artist\n",
        "    if os.path.isdir(\"lyrics/\" + artist):\n",
        "        shutil.rmtree(\"lyrics/\" + artist)\n",
        "    os.mkdir(\"lyrics/\" + artist)\n",
        "    path = \"lyrics/\" + artist\n",
        "\n",
        "    # 2. Iterate over the lyrics pages\n",
        "    # 3. Request the lyrics page.\n",
        "     # Don't forget to add a line like `time.sleep(5 + 10*random.random()) to sleep after making the request \n",
        "\n",
        "    for song in lyrics_pages[artist]:\n",
        "       \n",
        "        r = requests.get(song)\n",
        "        time.sleep(5 + 10*random.random())\n",
        "        lyrics_soup = BeautifulSoup(r.content)\n",
        "\n",
        "    # 4. Extract the title and lyrics from the page.\n",
        "        initial = lyrics_soup.find(\"b\")\n",
        "        title = initial.find_next(\"b\").get_text()\n",
        "        lyrics = lyrics_soup.find(\"div\", class_=False, id=False).get_text()\n",
        "\n",
        "    # 5. Write out the title, two returns ('\\n'), and the lyrics. Use `generate_filename_from_url to generate the filename. \n",
        "        lyrics_title = title + '\\n' + '\\n' + lyrics\n",
        "        open_path = os.path.join(path, generate_filename_from_link(song))\n",
        "        file = open(open_path,'w')\n",
        "        file.write(lyrics_title)\n",
        "\n",
        "    # Remember to pull at least 20 songs per artist. It may be fun to pull all the songs for the artist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "P9oAOCFep8Lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff86c640-9707-4c63-966a-2f19f318434c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total run time was 0.3 hours.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea3nJ0ueACjm"
      },
      "source": [
        "###**Evaluation**\n",
        "\n",
        "This assignment asks you to pull data from the Twitter API and scrape www.AZLyrics.com. After you have finished the above sections , run all the cells in this notebook. Print this to PDF and submit it, per the instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "LNqVGO0bAFcw"
      },
      "outputs": [],
      "source": [
        "# Simple word extractor from Peter Norvig: https://norvig.com/spell-correct.html\n",
        "def words(text): \n",
        "    return re.findall(r'\\w+', text.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zzXekXDAILr"
      },
      "source": [
        "###**Checking Twitter Data**\n",
        "\n",
        "\n",
        "The output from your Twitter API pull should be two files per artist, stored in files with formats like cher_followers.txt (a list of all follower IDs you pulled) and cher_followers_data.txt. These files should be in a folder named twitter within the repository directory. This code summarizes the information at a high level to help the instructor evaluate your work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "34N8BWCcAMCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6acf4fc-ab37-44b7-fb48-cd26e601908f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We see two artist handles: ecosmith and shayneTward.\n"
          ]
        }
      ],
      "source": [
        "twitter_files = os.listdir(\"twitter\")\n",
        "twitter_files = [f for f in twitter_files if f != \".DS_Store\"]\n",
        "artist_handles = ['ecosmith', 'shayneTward']\n",
        "artist_handles\n",
        "#artist_handles = list(set([name.split(\"_\")[0] for name in twitter_files]))\n",
        "\n",
        "print(f\"We see two artist handles: {artist_handles[0]} and {artist_handles[1]}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "artist_handles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_EzHOuxeWBN",
        "outputId": "48b14ef8-3234-408e-da13-3232883b0548"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ecosmith', 'shayneTward']"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for artist in artist_handles :\n",
        "    follower_file = artist + \"_followers.txt\"\n",
        "    follower_data_file = artist + \"_followers_data.txt\"\n",
        "    \n",
        "    ids = open(\"twitter/\" + follower_file,'r').readlines()\n",
        "    \n",
        "    print(f\"We see {len(ids)-1} in your follower file for {artist}, assuming a header row.\")\n",
        "    \n",
        "    with open(\"twitter/\" + follower_file,'r') as infile :\n",
        "        \n",
        "        # check the headers\n",
        "        headers = infile.readline().split(\"\\t\")\n",
        "        \n",
        "        print(f\"In the follower data file ({follower_data_file}) for {artist}, we have these columns:\")\n",
        "        print(\" : \".join(headers))\n",
        "        \n",
        "        description_words = []\n",
        "        locations = set()\n",
        "        \n",
        "        \n",
        "        for idx, line in enumerate(infile.readlines()) :\n",
        "            line = line.strip(\"\\n\").split(\"\\t\")\n",
        "            \n",
        "            try : \n",
        "                locations.add(line[3])            \n",
        "                description_words.extend(words(line[6]))\n",
        "            except :\n",
        "                pass\n",
        "    \n",
        "     \n",
        "        print(f\"We have {idx+1} data rows for {artist} in the follower data file.\")\n",
        "\n",
        "        print(f\"For {artist} we have {len(locations)} unique locations.\")\n",
        "\n",
        "        print(f\"For {artist} we have {len(description_words)} words in the descriptions.\")\n",
        "        print(\"Here are the five most common words:\")\n",
        "        print(Counter(description_words).most_common(5))\n",
        "\n",
        "        \n",
        "        print(\"\")\n",
        "        print(\"-\"*40)\n",
        "        print(\"\")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7npafw7ngA3o",
        "outputId": "b27be2ca-f2b0-4472-b66d-df9ecc046f4f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We see 100024 in your follower file for ecosmith, assuming a header row.\n",
            "In the follower data file (ecosmith_followers_data.txt) for ecosmith, we have these columns:\n",
            "ID : screen_name : name : location : follower_count : friends_count : description\n",
            "\n",
            "We have 100024 data rows for ecosmith in the follower data file.\n",
            "For ecosmith we have 13001 unique locations.\n",
            "For ecosmith we have 437174 words in the descriptions.\n",
            "Here are the five most common words:\n",
            "[('and', 13622), ('i', 10207), ('to', 8188), ('a', 8177), ('the', 7434)]\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "We see 100024 in your follower file for shayneTward, assuming a header row.\n",
            "In the follower data file (shayneTward_followers_data.txt) for shayneTward, we have these columns:\n",
            "ID : screen_name : name : location : follower_count : friends_count : description\n",
            "\n",
            "We have 100024 data rows for shayneTward in the follower data file.\n",
            "For shayneTward we have 13005 unique locations.\n",
            "For shayneTward we have 437147 words in the descriptions.\n",
            "Here are the five most common words:\n",
            "[('and', 13622), ('i', 10205), ('to', 8188), ('a', 8177), ('the', 7432)]\n",
            "\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfg6DqQYA035"
      },
      "source": [
        "###**Checking Lyrics**\n",
        "\n",
        "The output from your lyrics scrape should be stored in files located in this path from the directory: /lyrics/[Artist Name]/[filename from URL]. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "-Pl0gKKzA3XU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d57b075b-fbbc-4eeb-c5b9-5cb6713e22c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For ShayneWard we have 58 files.\n",
            "For ShayneWard we have roughly 17812 words, 1326 are unique.\n",
            "For EcoSmith we have 42 files.\n",
            "For EcoSmith we have roughly 11175 words, 1040 are unique.\n"
          ]
        }
      ],
      "source": [
        "artist_folders = os.listdir(\"lyrics/\")\n",
        "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
        "\n",
        "for artist in artist_folders : \n",
        "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
        "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
        "\n",
        "    print(f\"For {artist} we have {len(artist_files)} files.\")\n",
        "\n",
        "    artist_words = []\n",
        "\n",
        "    for f_name in artist_files : \n",
        "        with open(\"lyrics/\" + artist + \"/\" + f_name) as infile : \n",
        "            artist_words.extend(words(infile.read()))\n",
        "\n",
        "            \n",
        "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyMP/zz8xmpcFtnUQ0Inf6M2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}